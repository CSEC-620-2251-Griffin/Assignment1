# SMS Spam Classification - Performance Improvements Analysis

##  **Key Findings**

### **Performance Improvements Achieved:**

1. **k-NN Classifier: +44.66% F1-Score Improvement**
   - **Best Method**: Feature Selection (Chi-squared)
   - **F1-Score**: 0.7197 (vs 0.4975 original)
   - **Speed Improvement**: 44% faster (15.46s vs 27.74s)

2. **Naive Bayes Classifier: +0.82% F1-Score Improvement**
   - **Best Method**: Hyperparameter Tuned (α=2.0)
   - **F1-Score**: 0.9481 (vs 0.9404 original)
   - **Speed**: Maintained fast performance (0.01s)

##  **Detailed Results Comparison**

| Method | Model | Accuracy | Precision | Recall | F1-Score | Time (s) | Improvement |
|--------|-------|----------|-----------|--------|----------|----------|-------------|
| **Original** | k-NN | 0.9093 | 0.9615 | 0.3356 | 0.4975 | 27.74 | - |
| **Original** | Naive Bayes | 0.9847 | 0.9853 | 0.8993 | 0.9404 | 0.02 | - |
| **Enhanced Preprocessing** | k-NN | 0.8968 | 0.9048 | 0.2550 | 0.3979 | 25.79 | -20.0% |
| **Enhanced Preprocessing** | Naive Bayes | 0.9856 | 0.9854 | 0.9060 | 0.9441 | 0.01 | +0.4% |
| **Feature Selection (Chi2)** | k-NN | 0.9336 | 0.8261 | 0.6376 | **0.7197** | 15.46 | **+44.7%** |
| **Feature Selection (Chi2)** | Naive Bayes | 0.9856 | 0.9854 | 0.9060 | 0.9441 | 0.01 | +0.4% |
| **Optimized Naive Bayes** | Naive Bayes | 0.9820 | 0.9216 | 0.9463 | 0.9338 | 0.02 | -0.7% |
| **Hyperparameter Tuned** | Naive Bayes | 0.9865 | 0.9786 | 0.9195 | **0.9481** | 0.01 | **+0.8%** |

##  **Techniques Tested**

### 1. **Enhanced Text Preprocessing**
- **Stopword Removal**: Removed 100+ common English words
- **Stemming**: Applied simple stemming rules (ing, ed, s, ly, etc.)
- **Advanced Cleaning**: URL, email, phone number removal
- **Impact**: Reduced vocabulary size, but mixed results

### 2. **Feature Selection (Chi-squared)**
- **Method**: Chi-squared statistical test
- **Features Selected**: 1,000 from 6,611 (85% reduction)
- **Impact**: **Dramatic improvement for k-NN** (+44.7% F1-score)
- **Speed**: 44% faster execution

### 3. **Optimized Naive Bayes**
- **Feature Weighting**: IDF-based feature importance
- **Advanced Smoothing**: Different smoothing for seen/unseen words
- **Impact**: Slight performance variation

### 4. **Hyperparameter Tuning**
- **k-NN**: Tested k values (3, 5, 7, 9, 11)
- **Naive Bayes**: Tested α values (0.1, 0.5, 1.0, 2.0, 5.0)
- **Best Parameters**: k=5 (k-NN), α=2.0 (Naive Bayes)

##  **Key Insights**

### **Most Effective Techniques:**

1. **Feature Selection for k-NN**: 
   - **Why it works**: Removes noise features, focuses on discriminative words
   - **Result**: 44.7% F1-score improvement + 44% speed improvement
   - **Trade-off**: Slight precision decrease, major recall increase

2. **Hyperparameter Tuning for Naive Bayes**:
   - **Why it works**: α=2.0 provides better smoothing for this dataset
   - **Result**: 0.8% F1-score improvement
   - **Benefit**: Maintains fast speed while improving performance

### **Less Effective Techniques:**

1. **Enhanced Text Preprocessing**:
   - **Issue**: Over-aggressive preprocessing may remove important information
   - **Result**: Mixed results, sometimes worse performance
   - **Lesson**: Balance between cleaning and information preservation

2. **Feature Weighting in Naive Bayes**:
   - **Issue**: May over-emphasize rare words
   - **Result**: Slight performance decrease
   - **Lesson**: Simple approaches often work better

##  **Recommendations**

### **For Production Use:**

1. **Use Feature Selection for k-NN**: 
   - Chi-squared feature selection with k=1000 features
   - Provides best balance of performance and speed

2. **Use Hyperparameter Tuned Naive Bayes**:
   - α=2.0 for this dataset
   - Fast, accurate, and reliable

3. **Avoid Over-Preprocessing**:
   - Simple text cleaning is often sufficient
   - Stopword removal and stemming can hurt performance

### **For Further Research:**

1. **Advanced Feature Selection**: Mutual information, information gain
2. **Ensemble Methods**: Combine multiple classifiers
3. **Cross-Validation**: More robust hyperparameter tuning
4. **Different Distance Metrics**: Cosine similarity, Manhattan distance

## **Performance Summary**

- **Best k-NN**: Feature Selection (Chi2) - 0.7197 F1-score, 15.46s
- **Best Naive Bayes**: Hyperparameter Tuned - 0.9481 F1-score, 0.01s
- **Overall Winner**: Naive Bayes (faster and more accurate)
- **Biggest Improvement**: k-NN with feature selection (+44.7%)

The analysis demonstrates that **feature selection is the most impactful technique** for improving k-NN performance, while **hyperparameter tuning provides modest but consistent improvements** for Naive Bayes.
